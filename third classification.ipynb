{
 "cells": [
  {
   "cell_type": "code",
   "id": "28edc5f42785b9ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T03:22:32.803469Z",
     "start_time": "2025-01-16T03:22:32.777735Z"
    }
   },
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T03:22:45.122337Z",
     "start_time": "2025-01-16T03:22:32.803469Z"
    }
   },
   "source": [
    "import csv\n",
    "import torch\n",
    "fileName = 'data/loan_data.csv'\n",
    "\n",
    "with open(fileName) as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    variables = next(reader)\n",
    "    columnInfo = [[] for _ in range(len(variables))]\n",
    "    data = torch.tensor([])\n",
    "    labels = torch.tensor([])\n",
    "    for row in reader:\n",
    "        temp = torch.zeros(1, len(variables) - 1)\n",
    "        for columnIndex, column in enumerate(row[:-1]):\n",
    "            try:\n",
    "                value = float(column)\n",
    "            except ValueError:\n",
    "                if column not in columnInfo[columnIndex]:\n",
    "                    columnInfo[columnIndex].append(column)\n",
    "                value = columnInfo[columnIndex].index(column)\n",
    "            if value != 0:\n",
    "                value = value ** -1\n",
    "            temp[0][columnIndex] = value\n",
    "        data = torch.cat((data, temp))\n",
    "        labels = torch.cat((labels, torch.tensor([int(row[-1])])))\n",
    "labels = labels\n",
    "data.shape, labels.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([45000, 13]), torch.Size([45000]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "6cb129a90e9b70b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T03:22:45.153791Z",
     "start_time": "2025-01-16T03:22:45.123891Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_axis(variables[:-1], axis=1)\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       person_age  person_gender  person_education  person_income  \\\n",
       "0        0.045455            0.0          0.000000       0.000014   \n",
       "1        0.047619            0.0          1.000000       0.000081   \n",
       "2        0.040000            0.0          1.000000       0.000080   \n",
       "3        0.043478            0.0          0.500000       0.000013   \n",
       "4        0.041667            1.0          0.000000       0.000015   \n",
       "...           ...            ...               ...            ...   \n",
       "44995    0.037037            1.0          0.333333       0.000021   \n",
       "44996    0.027027            0.0          0.333333       0.000015   \n",
       "44997    0.030303            1.0          0.333333       0.000018   \n",
       "44998    0.034483            1.0          0.500000       0.000030   \n",
       "44999    0.041667            1.0          1.000000       0.000019   \n",
       "\n",
       "       person_emp_exp  person_home_ownership  loan_amnt  loan_intent  \\\n",
       "0            0.000000                    0.0   0.000029         0.00   \n",
       "1            0.000000                    1.0   0.001000         1.00   \n",
       "2            0.333333                    0.5   0.000182         0.50   \n",
       "3            0.000000                    0.0   0.000029         0.50   \n",
       "4            1.000000                    0.0   0.000029         0.50   \n",
       "...               ...                    ...        ...          ...   \n",
       "44995        0.166667                    0.0   0.000067         0.50   \n",
       "44996        0.058824                    0.0   0.000111         0.25   \n",
       "44997        0.142857                    0.0   0.000361         0.20   \n",
       "44998        0.250000                    0.0   0.000083         1.00   \n",
       "44999        1.000000                    0.0   0.000150         0.20   \n",
       "\n",
       "       loan_int_rate  loan_percent_income  cb_person_cred_hist_length  \\\n",
       "0           0.062422             2.040816                    0.333333   \n",
       "1           0.089767            12.500000                    0.500000   \n",
       "2           0.077700             2.272727                    0.333333   \n",
       "3           0.065660             2.272727                    0.500000   \n",
       "4           0.070077             1.886792                    0.250000   \n",
       "...              ...                  ...                         ...   \n",
       "44995       0.063857             3.225806                    0.333333   \n",
       "44996       0.071073             7.142857                    0.090909   \n",
       "44997       0.099800            20.000000                    0.100000   \n",
       "44998       0.075586             2.777778                    0.166667   \n",
       "44999       0.058651             7.692307                    0.333333   \n",
       "\n",
       "       credit_score  previous_loan_defaults_on_file  \n",
       "0          0.001783                             0.0  \n",
       "1          0.001984                             1.0  \n",
       "2          0.001575                             0.0  \n",
       "3          0.001481                             0.0  \n",
       "4          0.001706                             0.0  \n",
       "...             ...                             ...  \n",
       "44995      0.001550                             0.0  \n",
       "44996      0.001610                             0.0  \n",
       "44997      0.001497                             0.0  \n",
       "44998      0.001656                             0.0  \n",
       "44999      0.001592                             0.0  \n",
       "\n",
       "[45000 rows x 13 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_gender</th>\n",
       "      <th>person_education</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_exp</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_intent</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>previous_loan_defaults_on_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.062422</td>\n",
       "      <td>2.040816</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.089767</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.065660</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.070077</td>\n",
       "      <td>1.886792</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44995</th>\n",
       "      <td>0.037037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.063857</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44996</th>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.071073</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44997</th>\n",
       "      <td>0.030303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.099800</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44998</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.075586</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44999</th>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.058651</td>\n",
       "      <td>7.692307</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45000 rows × 13 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "948933f39b0ddeb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T03:22:45.177899Z",
     "start_time": "2025-01-16T03:22:45.153791Z"
    }
   },
   "source": [
    "pct = .8\n",
    "XSplit = int(pct*len(data))\n",
    "ySplit = int(pct * len(labels))\n",
    "XTrain, XTest = data[:XSplit].to(device), data[XSplit:].to(device)\n",
    "yTrain, yTest = labels[:ySplit].to(device), labels[ySplit:].to(device)\n",
    "XTrain.shape, XTest.shape, yTrain.shape, yTest.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([36000, 13]),\n",
       " torch.Size([9000, 13]),\n",
       " torch.Size([36000]),\n",
       " torch.Size([9000]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "475ca1adbe7b3abb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T03:22:45.222053Z",
     "start_time": "2025-01-16T03:22:45.178853Z"
    }
   },
   "source": [
    "XTrain[0:5], yTrain[0:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.5455e-02, 0.0000e+00, 0.0000e+00, 1.3899e-05, 0.0000e+00, 0.0000e+00,\n",
       "          2.8571e-05, 0.0000e+00, 6.2422e-02, 2.0408e+00, 3.3333e-01, 1.7825e-03,\n",
       "          0.0000e+00],\n",
       "         [4.7619e-02, 0.0000e+00, 1.0000e+00, 8.1420e-05, 0.0000e+00, 1.0000e+00,\n",
       "          1.0000e-03, 1.0000e+00, 8.9767e-02, 1.2500e+01, 5.0000e-01, 1.9841e-03,\n",
       "          1.0000e+00],\n",
       "         [4.0000e-02, 0.0000e+00, 1.0000e+00, 8.0399e-05, 3.3333e-01, 5.0000e-01,\n",
       "          1.8182e-04, 5.0000e-01, 7.7700e-02, 2.2727e+00, 3.3333e-01, 1.5748e-03,\n",
       "          0.0000e+00],\n",
       "         [4.3478e-02, 0.0000e+00, 5.0000e-01, 1.2539e-05, 0.0000e+00, 0.0000e+00,\n",
       "          2.8571e-05, 5.0000e-01, 6.5660e-02, 2.2727e+00, 5.0000e-01, 1.4815e-03,\n",
       "          0.0000e+00],\n",
       "         [4.1667e-02, 1.0000e+00, 0.0000e+00, 1.5121e-05, 1.0000e+00, 0.0000e+00,\n",
       "          2.8571e-05, 5.0000e-01, 7.0077e-02, 1.8868e+00, 2.5000e-01, 1.7065e-03,\n",
       "          0.0000e+00]], device='cuda:0'),\n",
       " tensor([1., 0., 1., 1., 1.], device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "ae6a24b124633f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T03:23:06.889053Z",
     "start_time": "2025-01-16T03:22:45.222053Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "class Model0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(13, 20)\n",
    "        self.layer2 = nn.Linear(20, 20)\n",
    "        self.layer3 = nn.Linear(20, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # return self.layer3(self.relu(self.layer2(self.relu(self.layer1(x)))))\n",
    "        return self.layer3(self.layer2(self.layer1(x)))\n",
    "\n",
    "\n",
    "def accuracy(targets, predictions):\n",
    "    correct = torch.eq(targets, predictions).sum().item()\n",
    "    acc = correct / len(targets)\n",
    "    return acc\n",
    "\n",
    "model = Model0().to(device)\n",
    "lossFun = nn.BCEWithLogitsLoss()\n",
    "losses = torch.tensor([])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "epochs = 10500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    yLogits = model(XTrain).squeeze()\n",
    "    yPred = torch.round(torch.sigmoid(yLogits))\n",
    "\n",
    "    trainLoss = lossFun(yLogits, yTrain)\n",
    "    losses = torch.cat((losses, trainLoss.unsqueeze(-1).to('cpu')))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    trainLoss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        testLogits = model(XTest).squeeze()\n",
    "        testPred = torch.round(torch.sigmoid(testLogits))\n",
    "        \n",
    "        testLoss = lossFun(testLogits, yTest)\n",
    "        testAcc = accuracy(yTest, testPred)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch:>5} | Train loss: {trainLoss:.4f} | Test loss: {testLoss:.4f}  | Test acc: {testAcc*100:.4f}%')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     0 | Train loss: 0.5360 | Test loss: 0.5879  | Test acc: 67.8667%\n",
      "Epoch:   100 | Train loss: 0.3721 | Test loss: 0.4821  | Test acc: 67.8667%\n",
      "Epoch:   200 | Train loss: 0.3538 | Test loss: 0.5645  | Test acc: 68.0889%\n",
      "Epoch:   300 | Train loss: 0.3366 | Test loss: 0.5214  | Test acc: 71.2556%\n",
      "Epoch:   400 | Train loss: 0.3270 | Test loss: 0.4922  | Test acc: 74.6111%\n",
      "Epoch:   500 | Train loss: 0.3208 | Test loss: 0.4719  | Test acc: 77.1667%\n",
      "Epoch:   600 | Train loss: 0.3165 | Test loss: 0.4568  | Test acc: 78.9444%\n",
      "Epoch:   700 | Train loss: 0.3132 | Test loss: 0.4451  | Test acc: 80.0000%\n",
      "Epoch:   800 | Train loss: 0.3106 | Test loss: 0.4358  | Test acc: 80.6000%\n",
      "Epoch:   900 | Train loss: 0.3085 | Test loss: 0.4282  | Test acc: 81.0111%\n",
      "Epoch:  1000 | Train loss: 0.3068 | Test loss: 0.4219  | Test acc: 81.4889%\n",
      "Epoch:  1100 | Train loss: 0.3053 | Test loss: 0.4165  | Test acc: 81.8000%\n",
      "Epoch:  1200 | Train loss: 0.3041 | Test loss: 0.4119  | Test acc: 81.9444%\n",
      "Epoch:  1300 | Train loss: 0.3030 | Test loss: 0.4080  | Test acc: 82.1778%\n",
      "Epoch:  1400 | Train loss: 0.3021 | Test loss: 0.4045  | Test acc: 82.3556%\n",
      "Epoch:  1500 | Train loss: 0.3012 | Test loss: 0.4015  | Test acc: 82.5333%\n",
      "Epoch:  1600 | Train loss: 0.3005 | Test loss: 0.3988  | Test acc: 82.5889%\n",
      "Epoch:  1700 | Train loss: 0.2998 | Test loss: 0.3964  | Test acc: 82.6889%\n",
      "Epoch:  1800 | Train loss: 0.2992 | Test loss: 0.3943  | Test acc: 82.8000%\n",
      "Epoch:  1900 | Train loss: 0.2987 | Test loss: 0.3923  | Test acc: 82.8667%\n",
      "Epoch:  2000 | Train loss: 0.2982 | Test loss: 0.3906  | Test acc: 82.9222%\n",
      "Epoch:  2100 | Train loss: 0.2977 | Test loss: 0.3890  | Test acc: 83.0667%\n",
      "Epoch:  2200 | Train loss: 0.2973 | Test loss: 0.3876  | Test acc: 83.1333%\n",
      "Epoch:  2300 | Train loss: 0.2969 | Test loss: 0.3863  | Test acc: 83.1778%\n",
      "Epoch:  2400 | Train loss: 0.2966 | Test loss: 0.3851  | Test acc: 83.1556%\n",
      "Epoch:  2500 | Train loss: 0.2962 | Test loss: 0.3840  | Test acc: 83.2444%\n",
      "Epoch:  2600 | Train loss: 0.2959 | Test loss: 0.3830  | Test acc: 83.3111%\n",
      "Epoch:  2700 | Train loss: 0.2956 | Test loss: 0.3821  | Test acc: 83.3667%\n",
      "Epoch:  2800 | Train loss: 0.2954 | Test loss: 0.3813  | Test acc: 83.4111%\n",
      "Epoch:  2900 | Train loss: 0.2951 | Test loss: 0.3805  | Test acc: 83.3889%\n",
      "Epoch:  3000 | Train loss: 0.2949 | Test loss: 0.3797  | Test acc: 83.4444%\n",
      "Epoch:  3100 | Train loss: 0.2946 | Test loss: 0.3790  | Test acc: 83.4556%\n",
      "Epoch:  3200 | Train loss: 0.2944 | Test loss: 0.3784  | Test acc: 83.4444%\n",
      "Epoch:  3300 | Train loss: 0.2942 | Test loss: 0.3778  | Test acc: 83.4556%\n",
      "Epoch:  3400 | Train loss: 0.2940 | Test loss: 0.3772  | Test acc: 83.4889%\n",
      "Epoch:  3500 | Train loss: 0.2938 | Test loss: 0.3767  | Test acc: 83.4778%\n",
      "Epoch:  3600 | Train loss: 0.2936 | Test loss: 0.3762  | Test acc: 83.5111%\n",
      "Epoch:  3700 | Train loss: 0.2935 | Test loss: 0.3757  | Test acc: 83.5222%\n",
      "Epoch:  3800 | Train loss: 0.2933 | Test loss: 0.3752  | Test acc: 83.5444%\n",
      "Epoch:  3900 | Train loss: 0.2931 | Test loss: 0.3748  | Test acc: 83.6000%\n",
      "Epoch:  4000 | Train loss: 0.2930 | Test loss: 0.3744  | Test acc: 83.5667%\n",
      "Epoch:  4100 | Train loss: 0.2928 | Test loss: 0.3740  | Test acc: 83.5778%\n",
      "Epoch:  4200 | Train loss: 0.2927 | Test loss: 0.3737  | Test acc: 83.5667%\n",
      "Epoch:  4300 | Train loss: 0.2926 | Test loss: 0.3733  | Test acc: 83.6333%\n",
      "Epoch:  4400 | Train loss: 0.2924 | Test loss: 0.3730  | Test acc: 83.6444%\n",
      "Epoch:  4500 | Train loss: 0.2923 | Test loss: 0.3727  | Test acc: 83.6556%\n",
      "Epoch:  4600 | Train loss: 0.2922 | Test loss: 0.3724  | Test acc: 83.6889%\n",
      "Epoch:  4700 | Train loss: 0.2921 | Test loss: 0.3721  | Test acc: 83.7222%\n",
      "Epoch:  4800 | Train loss: 0.2919 | Test loss: 0.3718  | Test acc: 83.7444%\n",
      "Epoch:  4900 | Train loss: 0.2918 | Test loss: 0.3715  | Test acc: 83.7778%\n",
      "Epoch:  5000 | Train loss: 0.2917 | Test loss: 0.3713  | Test acc: 83.7889%\n",
      "Epoch:  5100 | Train loss: 0.2916 | Test loss: 0.3710  | Test acc: 83.8333%\n",
      "Epoch:  5200 | Train loss: 0.2915 | Test loss: 0.3708  | Test acc: 83.8667%\n",
      "Epoch:  5300 | Train loss: 0.2914 | Test loss: 0.3706  | Test acc: 83.8667%\n",
      "Epoch:  5400 | Train loss: 0.2913 | Test loss: 0.3704  | Test acc: 83.8444%\n",
      "Epoch:  5500 | Train loss: 0.2912 | Test loss: 0.3701  | Test acc: 83.8444%\n",
      "Epoch:  5600 | Train loss: 0.2911 | Test loss: 0.3699  | Test acc: 83.8444%\n",
      "Epoch:  5700 | Train loss: 0.2910 | Test loss: 0.3697  | Test acc: 83.8333%\n",
      "Epoch:  5800 | Train loss: 0.2909 | Test loss: 0.3696  | Test acc: 83.8778%\n",
      "Epoch:  5900 | Train loss: 0.2908 | Test loss: 0.3694  | Test acc: 83.8667%\n",
      "Epoch:  6000 | Train loss: 0.2907 | Test loss: 0.3692  | Test acc: 83.8889%\n",
      "Epoch:  6100 | Train loss: 0.2907 | Test loss: 0.3690  | Test acc: 83.8889%\n",
      "Epoch:  6200 | Train loss: 0.2906 | Test loss: 0.3688  | Test acc: 83.9444%\n",
      "Epoch:  6300 | Train loss: 0.2905 | Test loss: 0.3687  | Test acc: 83.9333%\n",
      "Epoch:  6400 | Train loss: 0.2904 | Test loss: 0.3685  | Test acc: 83.9667%\n",
      "Epoch:  6500 | Train loss: 0.2903 | Test loss: 0.3684  | Test acc: 83.9556%\n",
      "Epoch:  6600 | Train loss: 0.2903 | Test loss: 0.3682  | Test acc: 84.0111%\n",
      "Epoch:  6700 | Train loss: 0.2902 | Test loss: 0.3681  | Test acc: 84.0333%\n",
      "Epoch:  6800 | Train loss: 0.2901 | Test loss: 0.3679  | Test acc: 84.0333%\n",
      "Epoch:  6900 | Train loss: 0.2900 | Test loss: 0.3678  | Test acc: 84.0222%\n",
      "Epoch:  7000 | Train loss: 0.2900 | Test loss: 0.3677  | Test acc: 84.0222%\n",
      "Epoch:  7100 | Train loss: 0.2899 | Test loss: 0.3675  | Test acc: 84.0444%\n",
      "Epoch:  7200 | Train loss: 0.2898 | Test loss: 0.3674  | Test acc: 84.0444%\n",
      "Epoch:  7300 | Train loss: 0.2897 | Test loss: 0.3673  | Test acc: 84.0556%\n",
      "Epoch:  7400 | Train loss: 0.2897 | Test loss: 0.3672  | Test acc: 84.0556%\n",
      "Epoch:  7500 | Train loss: 0.2896 | Test loss: 0.3671  | Test acc: 84.0667%\n",
      "Epoch:  7600 | Train loss: 0.2895 | Test loss: 0.3669  | Test acc: 84.0778%\n",
      "Epoch:  7700 | Train loss: 0.2895 | Test loss: 0.3668  | Test acc: 84.0889%\n",
      "Epoch:  7800 | Train loss: 0.2894 | Test loss: 0.3667  | Test acc: 84.1000%\n",
      "Epoch:  7900 | Train loss: 0.2893 | Test loss: 0.3666  | Test acc: 84.0889%\n",
      "Epoch:  8000 | Train loss: 0.2893 | Test loss: 0.3665  | Test acc: 84.0889%\n",
      "Epoch:  8100 | Train loss: 0.2892 | Test loss: 0.3664  | Test acc: 84.1222%\n",
      "Epoch:  8200 | Train loss: 0.2892 | Test loss: 0.3663  | Test acc: 84.1111%\n",
      "Epoch:  8300 | Train loss: 0.2891 | Test loss: 0.3662  | Test acc: 84.1111%\n",
      "Epoch:  8400 | Train loss: 0.2890 | Test loss: 0.3661  | Test acc: 84.1111%\n",
      "Epoch:  8500 | Train loss: 0.2890 | Test loss: 0.3660  | Test acc: 84.1222%\n",
      "Epoch:  8600 | Train loss: 0.2889 | Test loss: 0.3659  | Test acc: 84.1222%\n",
      "Epoch:  8700 | Train loss: 0.2889 | Test loss: 0.3658  | Test acc: 84.1333%\n",
      "Epoch:  8800 | Train loss: 0.2888 | Test loss: 0.3658  | Test acc: 84.1556%\n",
      "Epoch:  8900 | Train loss: 0.2887 | Test loss: 0.3657  | Test acc: 84.1444%\n",
      "Epoch:  9000 | Train loss: 0.2887 | Test loss: 0.3656  | Test acc: 84.1444%\n",
      "Epoch:  9100 | Train loss: 0.2886 | Test loss: 0.3655  | Test acc: 84.1444%\n",
      "Epoch:  9200 | Train loss: 0.2886 | Test loss: 0.3654  | Test acc: 84.1333%\n",
      "Epoch:  9300 | Train loss: 0.2885 | Test loss: 0.3653  | Test acc: 84.1333%\n",
      "Epoch:  9400 | Train loss: 0.2885 | Test loss: 0.3653  | Test acc: 84.1667%\n",
      "Epoch:  9500 | Train loss: 0.2884 | Test loss: 0.3652  | Test acc: 84.1667%\n",
      "Epoch:  9600 | Train loss: 0.2884 | Test loss: 0.3651  | Test acc: 84.1667%\n",
      "Epoch:  9700 | Train loss: 0.2883 | Test loss: 0.3650  | Test acc: 84.1556%\n",
      "Epoch:  9800 | Train loss: 0.2883 | Test loss: 0.3650  | Test acc: 84.1556%\n",
      "Epoch:  9900 | Train loss: 0.2882 | Test loss: 0.3649  | Test acc: 84.1556%\n",
      "Epoch: 10000 | Train loss: 0.2882 | Test loss: 0.3648  | Test acc: 84.1667%\n",
      "Epoch: 10100 | Train loss: 0.2881 | Test loss: 0.3648  | Test acc: 84.1778%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 38\u001B[0m\n\u001B[0;32m     36\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[1;32m---> 38\u001B[0m     testLogits \u001B[38;5;241m=\u001B[39m model(XTest)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m     39\u001B[0m     testPred \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mround(torch\u001B[38;5;241m.\u001B[39msigmoid(testLogits))\n\u001B[0;32m     41\u001B[0m     testLoss \u001B[38;5;241m=\u001B[39m lossFun(testLogits, yTest)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[33], line 12\u001B[0m, in \u001B[0;36mModel0.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# return self.layer3(self.relu(self.layer2(self.relu(self.layer1(x)))))\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer1(x)))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "6c732249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T03:23:06.890654Z",
     "start_time": "2025-01-16T03:23:06.890654Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.scatter(np.arange(len(losses), step=1), torch.detach(losses), 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d9fcb6265879064",
   "metadata": {},
   "source": [
    "# problems\n",
    "# I didn't know how to find the data\n",
    "# getting the data from the csv was a hassle\n",
    "# had to use reciprocals to make the numbers smaller.\n",
    "# not enough epochs\n",
    "# 0.1 lr worked, but the learning rate only got up to 80. \n",
    "# I had a faulty accuracy function\n",
    "# non-linear layers did not help (relu)\n",
    "# issues with devices and converting to numpy for matplotlib"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
